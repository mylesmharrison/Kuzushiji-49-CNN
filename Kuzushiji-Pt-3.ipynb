{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53e4653-3b7c-42a8-ad56-645c459e0815",
   "metadata": {
    "id": "89a1fc67-30eb-45d2-89da-32a6d950898b"
   },
   "source": [
    "# Kuzushiji-49: An Introduction to CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589a548-976b-44a3-bec9-3cc352a0abad",
   "metadata": {
    "id": "e6d26f1a-dd28-4b62-81f1-84c67ac26780"
   },
   "source": [
    "## Part 3: Training an AutoML Image Classification Model using Google Cloud Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b8f0b-5760-4917-944f-8a7e2d6906d3",
   "metadata": {
    "id": "26586eaa-ca81-43c4-b2b4-72b3b04331df"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "In [Part 1]() and [Part 2]() we were introduced to the fundamentals of computer vision and building a simple image classifier with a convolutional neural network (CNN) in Tensorflow and running on Google Colab.\n",
    "\n",
    "Now, because we are the modern day and age of cloud-computing and \"self-serve AI\", we don't actually need to train a model from scratch but can use managed services to do some [AutoML](https://en.wikipedia.org/wiki/Automated_machine_learning) for us. In this post I will do so using the [Vertex AI AutoML](https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide) offering on Google Cloud Platform (GCP).\n",
    "\n",
    "This post is in no way affiliated nor endorsed by Google, and though I choose to use GCP here, I should of course mention that there are equivalent offerings from the other major cloud providers where the same kind of work can be done with little to no coding experience:\n",
    "- [Amazon Sagemaker Autopilot](https://aws.amazon.com/sagemaker/autopilot/)\n",
    "- [Azure AutoML](https://azure.microsoft.com/en-ca/products/machine-learning/automatedml/)\n",
    "\n",
    "I'm really curious to see a couple things:\n",
    "\n",
    "1) Will Vertex AI's AutoML perform at all with the incredibly tiny images of this dataset?  \n",
    "- I would assume it will, though using AutoML on such a tiny dataset is probably massively overkill. Still, in this post, I am taking on the role of a \"citizen data scientist\" without experience building models from scratch.  \n",
    "2) Will there be any issues with using Japanese characters as class labels / file paths?  \n",
    "- I would hope not, as I assume GCP is big in Japan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147b2fc7-7807-4274-90bc-c45a89db9d8c",
   "metadata": {
    "id": "Cio2Cs6dMGoj"
   },
   "source": [
    "### Prepping the Data\n",
    "\n",
    "For computer vision models, Vertex AI [expects your data](https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data#csv_1) to be individual files in Google Cloud Storage (GCS) and an associated CSV with their paths and class labels. \n",
    "\n",
    "This required writing some simple code to iterate over the dataset and export all the images, since our data was contained in numpy arrays.\n",
    "\n",
    "First, we reload the data into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb01dc2-0200-42bf-acab-b9da336704d6",
   "metadata": {
    "id": "9c512e6e-6c1d-4211-8789-a94f8a37a377"
   },
   "outputs": [],
   "source": [
    "# Reload the data\n",
    "\n",
    "# Train\n",
    "X_train = np.load('data/k49-train-imgs.npz')['arr_0']\n",
    "y_train = np.load('data/k49-train-labels.npz')['arr_0']\n",
    "\n",
    "# Test\n",
    "X_test = np.load('data/k49-test-imgs.npz')['arr_0']\n",
    "y_test = np.load('data/k49-test-labels.npz')['arr_0']\n",
    "\n",
    "# Classmap\n",
    "classmap = pd.read_csv('data/k49_classmap.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205b628",
   "metadata": {},
   "source": [
    "Then, read in the classmap and create output directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87d0cfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the classmap and create the subfolders\n",
    "import os\n",
    "\n",
    "for i in classmap['char']:\n",
    "  \n",
    "    # Generate the subfolder filepath\n",
    "    filepath = f'output/img/{i}'\n",
    "    \n",
    "    # Create\n",
    "    os.mkdir(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef578c",
   "metadata": {},
   "source": [
    "Finally, we write out the image files. This step took approximately 10 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b8351a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 232365/232365 [09:59<00:00, 387.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(0, X_train.shape[0])):\n",
    "    \n",
    "    label = classmap.loc[y_train[i], 'char']\n",
    "    \n",
    "    filename = f'output/img/{label}/{i:06}.png'\n",
    "    #print(filename)\n",
    "\n",
    "    plt.imsave(filename, X_train[i, :, :], cmap='gray_r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbedd3b",
   "metadata": {},
   "source": [
    "And write out the class label file. I've assumed I'm going to create a Storage Bucket named `kuzushii49-mharrison':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0854078",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'gs://kuzushiji49-mharrison/'\n",
    "\n",
    "f = open('kuzushiji49_gcp.csv', 'w', encoding='utf8')\n",
    "\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    \n",
    "    label = classmap.loc[y_train[i], 'char']\n",
    "    \n",
    "    filename = f'output/img/{label}/{i:06}.png'\n",
    "    #print(filename)\n",
    "\n",
    "    gcp_path = base_path + filename + ',' + label\n",
    "    \n",
    "    f.write(gcp_path)\n",
    "    f.write('\\n')\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb49a3",
   "metadata": {},
   "source": [
    "Let's just double check the format of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03552850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gs://kuzushiji49-mharrison/output/img/ま/000000...</td>\n",
       "      <td>ま</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gs://kuzushiji49-mharrison/output/img/と/000001...</td>\n",
       "      <td>と</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gs://kuzushiji49-mharrison/output/img/な/000002...</td>\n",
       "      <td>な</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gs://kuzushiji49-mharrison/output/img/ま/000003...</td>\n",
       "      <td>ま</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gs://kuzushiji49-mharrison/output/img/く/000004...</td>\n",
       "      <td>く</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  gs://kuzushiji49-mharrison/output/img/ま/000000...  ま\n",
       "1  gs://kuzushiji49-mharrison/output/img/と/000001...  と\n",
       "2  gs://kuzushiji49-mharrison/output/img/な/000002...  な\n",
       "3  gs://kuzushiji49-mharrison/output/img/ま/000003...  ま\n",
       "4  gs://kuzushiji49-mharrison/output/img/く/000004...  く"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "df = pd.read_csv('kuzushiji49_gcp.csv', header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c4da2e",
   "metadata": {},
   "source": [
    "### Moving the Data to Google Cloud Storage\n",
    "\n",
    "Now that we have the necessary files, we need to create our storage bucket and upload the files. This can be done through the browser, but is also easily accomplished from the command line using [`gsutil`](https://cloud.google.com/storage/docs/gsutil). \n",
    "\n",
    "As [suggested in the documentation](https://cloud.google.com/storage/docs/gsutil/addlhelp/GlobalCommandLineOptions), we will use the `-m` flag for multi-threaded copy and also the `-q` flag to suppress output, since we are copying a very large number of files:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Set project\n",
    "gcloud config set project brilliant-will-391123\n",
    "\n",
    "# Create bucket\n",
    "gsutil mb gs://kuzushiji-49-mharrison\n",
    "\n",
    "# Upload the files\n",
    " gsutil -mq cp -r output/img gs://kuzushiji-49-mharrison/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71ba92",
   "metadata": {},
   "source": [
    "This step was quite time-consuming, as the total size of the final bucket is about 220MB of a very large number of small files, and took about an hour. We can check the operation was a success afterward by looking at the bucket contents in the console:\n",
    "\n",
    "![](./img/gsbucket.png)\n",
    "\n",
    "And also by using the command line:\n",
    "\n",
    "```bash\n",
    "gsutil du -sh gs://kuzushiji-49-mharrison/\n",
    "\n",
    "221.63 MiB   gs://kuzushiji-49-mharrison\n",
    "\n",
    "```\n",
    "\n",
    "Finally, we copy the metadata file to use with Vertex AI to the route of the bucket:\n",
    "\n",
    "```bash\n",
    "gsutil cp kuzushiji49_gcp.csv gs://kuzushiji-49-mharrison/\n",
    "\n",
    "Copying file://kuzushiji49_gcp.csv [Content-Type=application/vnd.ms-excel]...\n",
    "- [1 files][ 12.8 MiB/ 12.8 MiB]\n",
    "Operation completed over 1 objects/12.8 MiB.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5098f347",
   "metadata": {},
   "source": [
    "Next, in the GCP console, we navigate to Vertex AI, and click 'Enable All Recommended APIs':\n",
    "\n",
    "![](./img/vertexai_api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f505f32",
   "metadata": {},
   "source": [
    "Then click 'Datasets' and create a new dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc854c7",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
